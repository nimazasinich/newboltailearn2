// Persian BERT Processor for Legal Document Classification
// Real implementation with actual BERT operations

import { loadTensorFlow, PersianBertProcessorFallback } from './TensorFlowFallback';
import fs from 'fs';
import path from 'path';

export interface PersianLegalDocument {
    id: string;
    title: string;
    content: string;
    category: string;
    subcategory?: string;
    keywords?: string;
    legal_basis?: string;
    decision_summary?: string;
}

export interface BERTConfig {
    maxSequenceLength: number;
    vocabSize: number;
    hiddenSize: number;
    numLayers: number;
    numAttentionHeads: number;
    intermediateSize: number;
    dropout: number;
    learningRate: number;
}

export interface ClassificationResult {
    category: string;
    confidence: number;
    subcategory?: string;
    legalBasis?: string[];
    keywords?: string[];
}

export class PersianBertProcessor {
    private model: tf.LayersModel | null = null;
    private tokenizer: Map<string, number> = new Map();
    private categoryMap: Map<string, number> = new Map();
    private config: BERTConfig;
    private specialTokens = {
        CLS: 101,
        SEP: 102,
        PAD: 0,
        UNK: 100
    };

    constructor(config: BERTConfig) {
        this.config = config;
        this.initializeTokenizer();
    }

    // Initialize Persian BERT tokenizer
    private initializeTokenizer() {
        // Persian legal vocabulary with BERT-style subword tokenization
        const persianLegalTerms = [
            // Court terms
            'ÿØÿßÿØ⁄ØÿßŸá', 'ŸÇÿßÿ∂€å', 'ÿ±ÿß€å', 'ÿ≠⁄©ŸÖ', 'ŸÖÿ≠⁄©ŸàŸÖ€åÿ™', 'ÿÆŸàÿßŸáÿßŸÜ', 'ÿÆŸàÿßŸÜÿØŸá', 'ŸÖÿ™ŸáŸÖ',
            'ÿ¥ÿßŸáÿØ', 'ÿ¥ŸáÿßÿØÿ™', 'ŸÖÿØÿ±⁄©', 'ŸÖÿ≥ÿ™ŸÜÿØ', 'ŸÇÿßŸÜŸàŸÜ', 'ŸÖÿßÿØŸá', 'ÿ®ŸÜÿØ', 'ÿ™ÿ®ÿµÿ±Ÿá',
            
            // Legal categories
            'ÿ≠ŸÇŸàŸÇ', 'ŸÖÿØŸÜ€å', 'ÿ¨ÿ≤ÿß', 'ÿ™ÿ¨ÿßÿ±ÿ™', 'ÿÆÿßŸÜŸàÿßÿØŸá', '⁄©ÿßÿ±', 'ÿßÿØÿßÿ±€å', 'ŸÇÿ±ÿßÿ±ÿØÿßÿØ',
            
            // Civil law terms
            'ÿßÿ¨ÿßÿ±Ÿá', 'ÿÆÿ±€åÿØ', 'ŸÅÿ±Ÿàÿ¥', 'ŸÖÿßŸÑ⁄©€åÿ™', 'ÿßÿ±ÿ´', 'Ÿàÿµ€åÿ™', 'ŸàŸÇŸÅ', 'ÿ¥ÿ±⁄©ÿ™',
            
            // Criminal law terms
            'ÿ≥ÿ±ŸÇÿ™', '⁄©ŸÑÿßŸáÿ®ÿ±ÿØÿßÿ±€å', 'ÿÆ€åÿßŸÜÿ™_ÿØÿ±_ÿßŸÖÿßŸÜÿ™', 'ÿ∂ÿ±ÿ®_Ÿà_ÿ¨ÿ±ÿ≠', 'ŸÇÿ™ŸÑ', 'ŸÇÿ™ŸÑ_ÿπŸÖÿØ',
            'ŸÇÿ™ŸÑ_ÿ¥ÿ®Ÿá_ÿπŸÖÿØ', 'ŸÇÿ™ŸÑ_ÿÆÿ∑ÿß', 'ÿ≠ÿ®ÿ≥', 'ÿ™ÿπÿ≤€åÿ±€å', 'ŸÖÿ¨ÿßÿ≤ÿßÿ™', 'ÿ¨ÿ≤ÿß€å_ŸÜŸÇÿØ€å',
            
            // Family law terms
            'ÿ∑ŸÑÿßŸÇ', 'ÿ≠ÿ∂ÿßŸÜÿ™', 'ŸÜŸÅŸÇŸá', 'ŸÖŸáÿ±€åŸá', 'ÿßÿ≤ÿØŸàÿßÿ¨', 'ŸÜ⁄©ÿßÿ≠', 'ŸÅÿ≥ÿÆ', 'ÿ®ÿ∑ŸÑÿßŸÜ',
            
            // Labor law terms
            '⁄©ÿßÿ±⁄Øÿ±', '⁄©ÿßÿ±ŸÅÿ±ŸÖÿß', 'ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ', 'ÿßÿÆÿ±ÿßÿ¨', 'ÿßÿ∂ÿßŸÅŸá_⁄©ÿßÿ±€å', 'ŸÖÿ±ÿÆÿµ€å',
            'ÿ≠ŸÇŸàŸÇ_ŸÖÿπŸàŸÇ', 'ŸæÿßÿØÿßÿ¥', 'ÿ®€åŸÖŸá', 'ÿ®ÿßÿ≤ŸÜÿ¥ÿ≥ÿ™⁄Ø€å',
            
            // Commercial law terms
            'ÿ™ÿ¨ÿßÿ±€å', 'ÿ®ÿßÿ≤ÿ±⁄ØÿßŸÜ€å', 'ÿµŸÜÿπÿ™€å', 'ÿ™ŸàŸÑ€åÿØ€å', 'ÿÆÿØŸÖÿßÿ™€å', 'ÿ≥ŸáÿßŸÖ', 'ÿ≥ÿ±ŸÖÿß€åŸá',
            'ÿ≥ŸàÿØ', 'ÿ≤€åÿßŸÜ', 'ÿ™ÿ±ÿßÿ≤ŸÜÿßŸÖŸá', 'ÿµŸàÿ±ÿ™_ŸÖÿßŸÑ€å',
            
            // Administrative law terms
            'ÿØŸàŸÑÿ™', 'ÿßÿØÿßÿ±Ÿá', 'Ÿàÿ≤ÿßÿ±ÿ™', 'ÿßÿ≥ÿ™ÿßŸÜ', 'ÿ¥Ÿáÿ±ÿ≥ÿ™ÿßŸÜ', 'ÿ¥Ÿáÿ±ÿØÿßÿ±€å', 'ŸÖÿ¨ŸÑÿ≥',
            'ÿ±ÿ¶€åÿ≥_ÿ¨ŸÖŸáŸàÿ±', 'Ÿàÿ≤€åÿ±', 'ŸÖÿπÿßŸàŸÜ', 'ŸÖÿØ€åÿ±', 'ÿ±ÿ¶€åÿ≥'
        ];

        // Add Persian legal terms to vocabulary
        persianLegalTerms.forEach((term, index) => {
            this.tokenizer.set(term, index + 10); // Start from 10 to reserve special tokens
        });

        // Add common Persian words
        const commonPersianWords = [
            'ÿØÿ±', 'ÿßÿ≤', 'ÿ®Ÿá', 'ÿ®ÿß', '⁄©Ÿá', 'ÿß€åŸÜ', 'ÿ¢ŸÜ', 'ÿ®ÿ±ÿß€å', 'ÿ®ŸàÿØ', 'ÿßÿ≥ÿ™',
            'ŸÖ€å', 'ÿ±ÿß', 'ÿ™ÿß', '€åÿß', 'ŸáŸÖ', 'ŸÜ€åÿ≤', 'ŸáŸÖ⁄ÜŸÜ€åŸÜ', 'ŸÑÿ∞ÿß', 'ÿ®ŸÜÿßÿ®ÿ±ÿß€åŸÜ',
            'ŸÖŸàÿ±ÿØ', 'ÿÆÿµŸàÿµ', 'ŸÜÿ≥ÿ®ÿ™', 'ÿ∑ÿ®ŸÇ', 'ŸÖÿ∑ÿßÿ®ŸÇ', 'ÿ®ÿ±ÿßÿ≥ÿßÿ≥', 'ÿ®ÿ±ÿ∑ÿ®ŸÇ',
            'ÿ¥ÿØŸá', '⁄©ÿ±ÿØŸá', 'ÿ®ŸàÿØŸá', 'ÿÆŸàÿßŸáÿØ', 'ŸÖ€å‚Äåÿ¥ŸàÿØ', 'ŸÖ€å‚Äå⁄©ŸÜÿØ', 'ŸÖ€å‚Äåÿ®ÿßÿ¥ÿØ'
        ];

        commonPersianWords.forEach((word, index) => {
            this.tokenizer.set(word, persianLegalTerms.length + index + 10);
        });

        console.log(`‚úÖ Persian BERT tokenizer initialized with ${this.tokenizer.size} terms`);
    }

    // Tokenize Persian text with BERT-style subword tokenization
    private tokenizeText(text: string): number[] {
        // Simple BERT-style tokenization for Persian
        const tokens = text
            .replace(/[ÿåÿõ:ÿü!]/g, ' ') // Replace Persian punctuation
            .split(/\s+/)
            .filter(token => token.length > 0)
            .flatMap(token => {
                // Simple subword tokenization
                if (token.length > 4) {
                    // Split long words into subwords
                    const subwords = [];
                    for (let i = 0; i < token.length; i += 2) {
                        subwords.push(token.slice(i, i + 2));
                    }
                    return subwords;
                }
                return [token];
            })
            .map(token => {
                const normalized = token.toLowerCase().trim();
                return this.tokenizer.get(normalized) || this.specialTokens.UNK;
            });

        // Add CLS token at the beginning
        const tokenIds = [this.specialTokens.CLS, ...tokens];
        
        // Truncate or pad to max sequence length
        if (tokenIds.length > this.config.maxSequenceLength - 1) {
            tokenIds.splice(this.config.maxSequenceLength - 1);
        }
        
        // Add SEP token at the end
        tokenIds.push(this.specialTokens.SEP);
        
        // Pad to max sequence length
        while (tokenIds.length < this.config.maxSequenceLength) {
            tokenIds.push(this.specialTokens.PAD);
        }

        return tokenIds;
    }

    // Initialize category mapping
    private initializeCategoryMapping(documents: PersianLegalDocument[]) {
        const categories = [...new Set(documents.map(doc => doc.category))];
        categories.forEach((category, index) => {
            this.categoryMap.set(category, index);
        });
        console.log(`‚úÖ Category mapping initialized: ${categories.join(', ')}`);
    }

    // Create BERT model architecture
    private createBERTModel(): tf.LayersModel {
        const input = tf.input({ shape: [this.config.maxSequenceLength] });
        
        // Token embeddings
        const tokenEmbeddings = tf.layers.embedding({
            inputDim: this.config.vocabSize,
            outputDim: this.config.hiddenSize,
            inputLength: this.config.maxSequenceLength,
            maskZero: true
        }).apply(input) as tf.SymbolicTensor;

        // Position embeddings
        const positionEmbeddings = tf.layers.embedding({
            inputDim: this.config.maxSequenceLength,
            outputDim: this.config.hiddenSize
        }).apply(tf.range(0, this.config.maxSequenceLength).expandDims(0)) as tf.SymbolicTensor;

        // Combine embeddings
        const embeddings = tf.layers.add().apply([tokenEmbeddings, positionEmbeddings]) as tf.SymbolicTensor;
        
        // Layer normalization
        let hiddenStates = tf.layers.layerNormalization().apply(embeddings) as tf.SymbolicTensor;

        // BERT transformer layers
        for (let i = 0; i < this.config.numLayers; i++) {
            hiddenStates = this.createTransformerLayer(hiddenStates, i);
        }

        // Classification head
        const pooledOutput = tf.layers.globalAveragePooling1d().apply(hiddenStates) as tf.SymbolicTensor;
        const dropout = tf.layers.dropout({ rate: this.config.dropout }).apply(pooledOutput) as tf.SymbolicTensor;
        const classifier = tf.layers.dense({
            units: this.categoryMap.size,
            activation: 'softmax'
        }).apply(dropout) as tf.SymbolicTensor;

        const model = tf.model({ inputs: input, outputs: classifier });

        // Compile model
        model.compile({
            optimizer: tf.train.adam(this.config.learningRate),
            loss: 'categoricalCrossentropy',
            metrics: ['accuracy']
        });

        console.log('‚úÖ Persian BERT model architecture created');
        return model;
    }

    // Create transformer layer
    private createTransformerLayer(input: tf.SymbolicTensor, layerIndex: number): tf.SymbolicTensor {
        // Multi-head self-attention
        const attentionOutput = tf.layers.multiHeadAttention({
            numHeads: this.config.numAttentionHeads,
            keyDim: this.config.hiddenSize / this.config.numAttentionHeads
        }).apply([input, input]) as tf.SymbolicTensor;

        // Add & Norm
        const attentionNorm = tf.layers.add().apply([input, attentionOutput]) as tf.SymbolicTensor;
        const attentionLayerNorm = tf.layers.layerNormalization().apply(attentionNorm) as tf.SymbolicTensor;

        // Feed-forward network
        const ffn = tf.layers.dense({
            units: this.config.intermediateSize,
            activation: 'relu'
        }).apply(attentionLayerNorm) as tf.SymbolicTensor;

        const ffnOutput = tf.layers.dense({
            units: this.config.hiddenSize
        }).apply(ffn) as tf.SymbolicTensor;

        // Add & Norm
        const ffnNorm = tf.layers.add().apply([attentionLayerNorm, ffnOutput]) as tf.SymbolicTensor;
        const output = tf.layers.layerNormalization().apply(ffnNorm) as tf.SymbolicTensor;

        return output;
    }

    // Prepare training data
    private prepareTrainingData(documents: PersianLegalDocument[]): { x: tf.Tensor, y: tf.Tensor } {
        const tokenizedTexts: number[][] = [];
        const labels: number[] = [];

        documents.forEach(doc => {
            const tokens = this.tokenizeText(doc.content);
            tokenizedTexts.push(tokens);
            
            const categoryIndex = this.categoryMap.get(doc.category);
            if (categoryIndex !== undefined) {
                labels.push(categoryIndex);
            }
        });

        // Convert to tensors
        const x = tf.tensor2d(tokenizedTexts);
        const y = tf.oneHot(tf.tensor1d(labels, 'int32'), this.categoryMap.size);

        console.log(`‚úÖ Training data prepared: ${documents.length} documents, ${this.categoryMap.size} categories`);
        return { x, y };
    }

    // Train the model
    async train(
        documents: PersianLegalDocument[],
        onProgress?: (progress: any) => void
    ): Promise<tf.History> {
        try {
            console.log(`üöÄ Starting Persian BERT training with ${documents.length} documents`);

            // Initialize category mapping
            this.initializeCategoryMapping(documents);

            // Create model
            this.model = this.createBERTModel();

            // Prepare data
            const { x, y } = this.prepareTrainingData(documents);

            // Split data for validation
            const validationSplit = 0.2;
            const validationSize = Math.floor(documents.length * validationSplit);
            const trainSize = documents.length - validationSize;

            const xTrain = x.slice([0, 0], [trainSize, this.config.maxSequenceLength]);
            const yTrain = y.slice([0, 0], [trainSize, this.categoryMap.size]);
            const xVal = x.slice([trainSize, 0], [validationSize, this.config.maxSequenceLength]);
            const yVal = y.slice([trainSize, 0], [validationSize, this.categoryMap.size]);

            console.log(`üìä Training split: ${trainSize} train, ${validationSize} validation`);

            // Training callbacks
            const callbacks = {
                onEpochEnd: (epoch: number, logs: any) => {
                    console.log(`üìà Epoch ${epoch + 1} - Loss: ${logs.loss.toFixed(4)}, Accuracy: ${(logs.accuracy * 100).toFixed(2)}%`);
                    
                    if (onProgress) {
                        onProgress({
                            epoch: epoch + 1,
                            loss: logs.loss,
                            accuracy: logs.accuracy,
                            valLoss: logs.val_loss,
                            valAccuracy: logs.val_accuracy
                        });
                    }
                }
            };

            // Train the model
            const history = await this.model.fit(xTrain, yTrain, {
                epochs: 10,
                batchSize: 8, // Smaller batch size for BERT
                validationData: [xVal, yVal],
                callbacks: callbacks,
                verbose: 1
            });

            // Clean up tensors
            x.dispose();
            y.dispose();
            xTrain.dispose();
            yTrain.dispose();
            xVal.dispose();
            yVal.dispose();

            console.log('‚úÖ Persian BERT training completed successfully');
            return history;

        } catch (error) {
            console.error('‚ùå Persian BERT training failed:', error);
            throw error;
        }
    }

    // Classify document
    async classify(document: PersianLegalDocument): Promise<ClassificationResult> {
        if (!this.model) {
            throw new Error('Model not trained yet');
        }

        const tokens = this.tokenizeText(document.content);
        const input = tf.tensor2d([tokens]);
        
        const prediction = this.model.predict(input) as tf.Tensor;
        const probabilities = await prediction.data();
        
        // Find highest probability category
        let maxIndex = 0;
        let maxProb = probabilities[0];
        
        for (let i = 1; i < probabilities.length; i++) {
            if (probabilities[i] > maxProb) {
                maxProb = probabilities[i];
                maxIndex = i;
            }
        }

        // Get category name from index
        const categoryName = Array.from(this.categoryMap.keys())[maxIndex];
        
        // Extract legal basis and keywords
        const legalBasis = this.extractLegalBasis(document.content);
        const keywords = this.extractKeywords(document.content);
        
        input.dispose();
        prediction.dispose();

        return {
            category: categoryName,
            confidence: maxProb,
            subcategory: document.subcategory,
            legalBasis,
            keywords
        };
    }

    // Extract legal basis from document content
    private extractLegalBasis(content: string): string[] {
        const legalBasisPatterns = [
            /ŸÖÿßÿØŸá\s+(\d+)/g,
            /ÿ®ŸÜÿØ\s+(\d+)/g,
            /ÿ™ÿ®ÿµÿ±Ÿá\s+(\d+)/g,
            /ŸÇÿßŸÜŸàŸÜ\s+([^ÿå]+)/g,
            /ŸÖŸÇÿ±ÿ±ÿßÿ™\s+([^ÿå]+)/g
        ];

        const legalBasis: string[] = [];
        
        legalBasisPatterns.forEach(pattern => {
            const matches = content.match(pattern);
            if (matches) {
                legalBasis.push(...matches);
            }
        });

        return [...new Set(legalBasis)]; // Remove duplicates
    }

    // Extract keywords from document content
    private extractKeywords(content: string): string[] {
        const keywords: string[] = [];
        
        // Extract legal terms
        this.tokenizer.forEach((tokenId, term) => {
            if (content.includes(term) && term.length > 2) {
                keywords.push(term);
            }
        });

        // Sort by frequency and return top keywords
        return keywords
            .sort((a, b) => {
                const countA = (content.match(new RegExp(a, 'g')) || []).length;
                const countB = (content.match(new RegExp(b, 'g')) || []).length;
                return countB - countA;
            })
            .slice(0, 10); // Top 10 keywords
    }

    // Save model
    async saveModel(modelPath: string): Promise<void> {
        if (!this.model) {
            throw new Error('Model not trained yet');
        }

        try {
            const dir = path.dirname(modelPath);
            if (!fs.existsSync(dir)) {
                fs.mkdirSync(dir, { recursive: true });
            }

            await this.model.save(`file://${modelPath}`);
            
            const metadata = {
                tokenizer: Object.fromEntries(this.tokenizer),
                categoryMap: Object.fromEntries(this.categoryMap),
                config: this.config,
                specialTokens: this.specialTokens
            };

            fs.writeFileSync(`${modelPath}_metadata.json`, JSON.stringify(metadata, null, 2));
            
            console.log(`‚úÖ Persian BERT model saved to ${modelPath}`);
        } catch (error) {
            console.error('‚ùå Failed to save model:', error);
            throw error;
        }
    }

    // Load model
    async loadModel(modelPath: string): Promise<void> {
        try {
            this.model = await tf.loadLayersModel(`file://${modelPath}/model.json`);
            
            const metadataPath = `${modelPath}_metadata.json`;
            if (fs.existsSync(metadataPath)) {
                const metadata = JSON.parse(fs.readFileSync(metadataPath, 'utf8'));
                
                this.tokenizer = new Map(Object.entries(metadata.tokenizer));
                this.categoryMap = new Map(Object.entries(metadata.categoryMap));
                this.config = metadata.config;
                this.specialTokens = metadata.specialTokens;
                
                console.log(`‚úÖ Persian BERT model loaded from ${modelPath}`);
            } else {
                throw new Error('Model metadata not found');
            }
        } catch (error) {
            console.error('‚ùå Failed to load model:', error);
            throw error;
        }
    }

    // Get model info
    getModelInfo(): any {
        return {
            vocabSize: this.tokenizer.size,
            categories: this.categoryMap.size,
            config: this.config,
            specialTokens: this.specialTokens
        };
    }
}

export default PersianBertProcessor;
